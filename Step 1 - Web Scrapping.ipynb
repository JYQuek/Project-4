{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping \n",
    "\n",
    "In this notebook, I will be scraping using Selenium and Beautiful Soup. \n",
    "\n",
    "The website that I will be scraping is https://www.mycareersfuture.sg/, which is the newest Singapore government initiative to help Singaporeans with a smarter way to find jobs. I am looking to retrieve job listings for data related positions in order to study factors (e.g. salaries) regarding data related positions in Singapore. \n",
    "\n",
    "The steps for web scraping are recorded below in detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "### Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the packages required\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrappy\n",
    "\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "### Study the structure of the web pages at MyCareersFuture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the pages are as such:\n",
    "\n",
    "__Main home page__, where we can enter the search term (i.e. job titles we are interested in)\n",
    "\n",
    "                THEN\n",
    "\n",
    "This returns the __search results pages__, where it shows the job listings for the search term\n",
    "\n",
    "                THEN \n",
    "                \n",
    "If we click on each job listing on the search results page, it returns the __detailed information for the job listing__ e.g. salary, employment type, job description and requirements etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, in order to scrape the detailed job listings, we have to follow the steps:\n",
    "1. Set up the urls for the search results page for each search term\n",
    "2. Use Selenihm webdriver to go to each search results page to retrieve html for the page\n",
    "3. From the html retrieved, use Beautiful Soup to extract the url for the individual job listings\n",
    "4. Use Selenihm webdriver to go to each job listing page to retrieve html for the page\n",
    "5. From the html retrieved, use Beautiful Soup to extract the details of the job listing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have extracted the details of each job listing:\n",
    "\n",
    "1. Load all the details into a Pandas DataFrame\n",
    "2. Export the data as a csv file to be stored for future usage in another notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "### Determine the job titles that I am interested in for my study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the job titles related to data, that I plan to scrape job listings for:\n",
    "    # Data Analyst\n",
    "    # Data Scientist\n",
    "    # Data Engineer\n",
    "    # Data Architect\n",
    "    # Data Manager\n",
    "    # Data Developer\n",
    "    # Business Intelligence Analyst\n",
    "    # Business Analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "### Consolidate URLs for the search results of the different job titles\n",
    "\n",
    "</font>\n",
    "\n",
    "I will consolidate the URLs for the search results for different job titles on MyCareersFuture. This way, I can run these URLs using the Selenium webdriver later, to retrieve the HTML for all the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract URLs for search results of each job title\n",
    "# Data Analyst - Search results\n",
    "\n",
    "data_analyst = []\n",
    "\n",
    "for i in range(0,4):\n",
    "    data_analyst.append('https://www.mycareersfuture.sg/search?search=data%20analyst&sortBy=new_posting_date&page='+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract URLs for search results of each job title\n",
    "# Data Scientist - Search results\n",
    "\n",
    "data_scientist = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    data_scientist.append('https://www.mycareersfuture.sg/search?search=data%20scientist&sortBy=new_posting_date&page='+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract URLs for search results of each job title\n",
    "# Data Engineer - Search results\n",
    "\n",
    "data_engineer = []\n",
    "\n",
    "for i in range(0,7):\n",
    "    data_engineer.append('https://www.mycareersfuture.sg/search?search=Data%20engineer&sortBy=new_posting_date&page='+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract URLs for search results of each job title\n",
    "# Data Architect - Search results\n",
    "\n",
    "data_architect = []\n",
    "\n",
    "for i in range(0,2):\n",
    "    data_architect.append('https://www.mycareersfuture.sg/search?search=Data%20Architect&sortBy=new_posting_date&page='+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract URLs for search results of each job title\n",
    "# Data Manager - Search results\n",
    "\n",
    "data_manager = []\n",
    "\n",
    "for i in range(0,3):\n",
    "    data_manager.append('https://www.mycareersfuture.sg/search?search=Data%20manager&sortBy=new_posting_date&page='+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract URLs for search results of each job title\n",
    "# Data Developer - Search results\n",
    "\n",
    "data_developer = []\n",
    "\n",
    "for i in range(0,1):\n",
    "    data_developer.append('https://www.mycareersfuture.sg/search?search=Data%20developer&sortBy=new_posting_date&page='+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract URLs for search results of each job title\n",
    "# Business Intelligence Analyst - Search results\n",
    "\n",
    "business_intelligence_analyst = []\n",
    "\n",
    "for i in range(0,1):\n",
    "    business_intelligence_analyst.append('https://www.mycareersfuture.sg/search?search=business%20intelligence%20analyst&sortBy=new_posting_date&page='+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract URLs for search results of each job title\n",
    "# Business Analyst - Search results\n",
    "\n",
    "business_analyst = []\n",
    "\n",
    "for i in range(0,18):\n",
    "    business_analyst.append('https://www.mycareersfuture.sg/search?search=business%20analyst&sortBy=new_posting_date&page='+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate all the urls into one single list for iteration using selenium below\n",
    "\n",
    "consol_list = data_analyst + data_scientist + data_engineer + data_architect + data_manager + data_developer \\\n",
    "                + business_intelligence_analyst + business_analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "### Open Selenium Webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "\n",
    "chromedriver = \"/Users/JooYeng/Downloads/chromedriver 4\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "# Create a driver called \"driver.\"\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/JooYeng/Downloads/chromedriver 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>\n",
    "\n",
    "### Use Selenium to extract html for the search result pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use selenium to extract html for the consol_list\n",
    "\n",
    "html = []\n",
    "\n",
    "for page in consol_list:\n",
    "    driver.get(page)\n",
    "    sleep(4)\n",
    "    html.append(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>\n",
    "\n",
    "### Use Beautiful Soup to extract url of each job listing page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/job/mid-level-data-analyst-traveloka-services-0b8be8fb990807c9e3e2f035cd12504a\n",
      "/job/platform-engineer-gumi-asia-54f313f4b63eb6c0fcb838a51cd8b318\n",
      "/job/data-analytics-lead-traveloka-services-8712611c3fb3ab76d7cadf931f3983fa\n",
      "/job/senior-data-analyst-propertyguru-707f35425322b89a7451c4f0316250d4\n",
      "/job/data-engineer-2fab625fb13fe16207621f894b9ebdb9\n",
      "/job/crm-data-analyst-aspire-global-network-72550b8ea34bcc4bb37f4efba636afd6\n",
      "/job/crm-data-analyst-aspire-global-network-dccc8a3b7b4f2f1946c7e0d24ea01269\n",
      "/job/technical-data-analyst-eclerx-7cc809458abebca3475715b2f588438f\n",
      "/job/data-analyst-emerio-globesoft-e0e7e3098911377e04e4dfc531402529\n",
      "/job/data-management-analyst-operations-unilite-recruitment-services-565d5ed6978c091752d10f8391774e28\n",
      "/job/data-scientist-merck-282468cd9f95fb3a2711b22f64ac37a4\n",
      "/job/supply-chain-planning-anaylst-konica-minolta-business-solutions-asia-893c163f3c5cec8bf6224390b8c06ac4\n",
      "/job/data-analyst-ntuc-enterprise-co-operative-81a4fdffd3d1823c4701e53227d7de71\n",
      "/job/business-analyst-ntuc-enterprise-co-operative-020e2a9bb2f95f34a656214bc96b2c9a\n",
      "/job/analyst-static-data-maintenance-specialist-sg-wealth-mgmt-ops-to-dbs-bank-e2c1bdf1719ef395999aca946c43f94b\n",
      "/job/data-analyst-69b5c709d9a8e822305ea222a2611bdc\n",
      "/job/data-analyst-99-2c1cd1db7ee7ac80264d94b0dc01481b\n",
      "/job/senior-data-analyst-99-4dcdad3423c05d06ef994cf250433207\n",
      "/job/senior-business-data-analyst-standard-chartered-bank-7e035c966e08bfef85b61180e180e04b\n",
      "/job/business-analyst-bluechip-platforms-asia-a41aba0635c055b13585edbfd9c70a98\n"
     ]
    }
   ],
   "source": [
    "# Check how to use Beautiful Soup to extract the url\n",
    "\n",
    "pg = BeautifulSoup(html[0]).findAll(\"a\",{\"class\":\"bg-white mb3 w-100 dib v-top pa3 no-underline flex-ns flex-wrap JobCard__card___22xP3\"})\n",
    "for job in pg:\n",
    "    print(job.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the href for each individual job \n",
    "# Consolidate them into a list\n",
    "\n",
    "url_part = []\n",
    "\n",
    "for page in html:\n",
    "    pg = BeautifulSoup(page).findAll(\"a\",{\"class\":\"bg-white mb3 w-100 dib v-top pa3 no-underline flex-ns flex-wrap JobCard__card___22xP3\"})\n",
    "    for job in pg:\n",
    "        url_part.append(job.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the href extracted is only part of the url, I will engineer the results to \n",
    "# get the full url for each individual job\n",
    "\n",
    "url_full = []\n",
    "\n",
    "for url in url_part:\n",
    "    url_full.append('https://www.mycareersfuture.sg'+url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>\n",
    "\n",
    "### Use Selenium to extract html for each job listing page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Selenium webdriver\n",
    "\n",
    "chromedriver = \"/Users/JooYeng/Downloads/chromedriver 4\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "# Create a driver called \"driver.\"\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/JooYeng/Downloads/chromedriver 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the url list to retrieve detailed job requirements for each of the job listing\n",
    "\n",
    "job_html = []\n",
    "\n",
    "for job in url_full:\n",
    "    driver.get(job)\n",
    "    sleep(5)\n",
    "    job_html.append(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>\n",
    "\n",
    "### Use Beautiful Soup to extract job details from each job listing page html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the information that I am interested in are:\n",
    "    - Company Name\n",
    "    - Job ID\n",
    "    - Full Job Title\n",
    "    - Employment Type (Permanent, Contract etc.)\n",
    "    - Seniority\n",
    "    - Job Category/Industry\n",
    "    - Salary Amount\n",
    "    - Job Description\n",
    "    - Job Requirements\n",
    "    - Number of applicants\n",
    "    - Posted Date of listing\n",
    "    - Expiry Date of listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append job details for each job\n",
    "\n",
    "comp = []\n",
    "job_id = []\n",
    "title = []\n",
    "emp_type = []\n",
    "seniority = []\n",
    "job_cat = []\n",
    "\n",
    "for job in job_html:\n",
    "    try:\n",
    "        comp.append(BeautifulSoup(job).find(\"p\",{\"name\":\"company\"}).text)\n",
    "    except AttributeError:\n",
    "        comp.append(np.nan)\n",
    "    try:    \n",
    "        job_id.append(BeautifulSoup(job).find(\"span\",{\"class\":\"black-60 db f6 fw4 mv1\"}).text)\n",
    "    except AttributeError:\n",
    "        job_id.append(np.nan)\n",
    "    try:\n",
    "        title.append(BeautifulSoup(job).find(\"h1\",{\"id\":\"job_title\"}).text)\n",
    "    except AttributeError:\n",
    "        title.append(np.nan)\n",
    "    try:\n",
    "        emp_type.append(BeautifulSoup(job).find(\"p\",{\"id\":\"employment_type\"}).text)\n",
    "    except AttributeError:\n",
    "        emp_type.append(np.nan)\n",
    "    try:        \n",
    "        seniority.append(BeautifulSoup(job).find(\"p\",{\"id\":\"seniority\"}).text)\n",
    "    except AttributeError:\n",
    "        seniority.append(np.nan)\n",
    "    try:\n",
    "        job_cat.append(BeautifulSoup(job).find(\"p\",{\"id\":\"job-categories\"}).text)\n",
    "    except AttributeError:\n",
    "        job_cat.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append job details for each job\n",
    "\n",
    "job_desc = []\n",
    "job_req = []\n",
    "\n",
    "for job in job_html:\n",
    "    try:\n",
    "        job_desc.append(BeautifulSoup(job).find(\"div\",{\"id\":\"description-content\"}).text)\n",
    "    except AttributeError:\n",
    "        job_desc.append(np.nan)\n",
    "    try:\n",
    "        job_req.append(BeautifulSoup(job).find(\"div\",{\"id\":\"requirements-content\"}).text)\n",
    "    except AttributeError:\n",
    "        job_req.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append job details for each job\n",
    "\n",
    "num_app = []\n",
    "posted_date = []\n",
    "exp_date = []\n",
    "\n",
    "for job in job_html:\n",
    "    try:\n",
    "        num_app.append(BeautifulSoup(job).find(\"span\",{\"id\":\"num_of_applications\"}).text)\n",
    "    except AttributeError:\n",
    "        num_app.append(np.nan)\n",
    "    try:\n",
    "        posted_date.append(BeautifulSoup(job).find(\"span\",{\"id\":\"last_posted_date\"}).text)\n",
    "    except AttributeError:\n",
    "        posted_date.append(np.nan)\n",
    "    try:\n",
    "        exp_date.append(BeautifulSoup(job).find(\"span\",{\"id\":\"expiry_date\"}).text)\n",
    "    except AttributeError:\n",
    "        exp_date.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append job details for each job\n",
    "\n",
    "sal_amt = []\n",
    "sal_term = []\n",
    "\n",
    "for job in job_html:\n",
    "    try:\n",
    "        sal_amt.append(BeautifulSoup(job).find(\"div\",{\"class\":\"lh-solid\"}).text)\n",
    "    except AttributeError:\n",
    "        sal_amt.append(np.nan)\n",
    "    try:\n",
    "        sal_term.append(BeautifulSoup(job).find(\"span\",{\"class\":\"salary_type dib f5 fw4 black-60 pr1 i pb\"}).text)\n",
    "    except AttributeError:\n",
    "        sal_term.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "### Create a DataFrame with all the information extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = []\n",
    "emp_type = []\n",
    "num_app = []\n",
    "posted_date = []\n",
    "exp_date = []\n",
    "sal_amt = []\n",
    "sal_term = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with all the information\n",
    "\n",
    "    # Create lists with the keys and values to zip the 2 lists together\n",
    "dict_key_list = ['job_id','job_title','url','job_desc','job_req','yrs_exp','indus',\n",
    "                 'comp','sal_amt','sal_term','emp_type','num_app','posted_date','exp_date']\n",
    "dict_val_list = [job_id,title,url_full,job_desc,job_req,seniority,job_cat,\n",
    "                comp,sal_amt,sal_term,emp_type,num_app,posted_date,exp_date]\n",
    "\n",
    "    # Zip the 2 lists together and create a dictionary\n",
    "jobs_dict = dict(zip(dict_key_list,dict_val_list))\n",
    "\n",
    "    # Create a dataframe from the dictionary\n",
    "jobs_df = pd.DataFrame(jobs_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>url</th>\n",
       "      <th>job_desc</th>\n",
       "      <th>job_req</th>\n",
       "      <th>yrs_exp</th>\n",
       "      <th>indus</th>\n",
       "      <th>comp</th>\n",
       "      <th>sal_amt</th>\n",
       "      <th>sal_term</th>\n",
       "      <th>emp_type</th>\n",
       "      <th>num_app</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>exp_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JOB-2019-0089696</td>\n",
       "      <td>Mid Level Data Analyst</td>\n",
       "      <td>https://www.mycareersfuture.sg/job/mid-level-d...</td>\n",
       "      <td>Data Analyst at Traveloka is at the forefront ...</td>\n",
       "      <td>We are looking for someone with:   Passion in ...</td>\n",
       "      <td>Junior Executive</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>TRAVELOKA SERVICES PTE. LTD.</td>\n",
       "      <td>$4,300to$7,600</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>Permanent</td>\n",
       "      <td>2 applications</td>\n",
       "      <td>Posted 26 Apr 2019</td>\n",
       "      <td>Closing on 26 May 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JOB-2019-0089321</td>\n",
       "      <td>Platform Engineer</td>\n",
       "      <td>https://www.mycareersfuture.sg/job/platform-en...</td>\n",
       "      <td>Responsible for programming and maintaining mo...</td>\n",
       "      <td>Coding/Scripting experience in languages such...</td>\n",
       "      <td>Executive</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>GUMI ASIA PTE. LTD.</td>\n",
       "      <td>$5,000to$6,500</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>0 application</td>\n",
       "      <td>Posted 26 Apr 2019</td>\n",
       "      <td>Closing on 26 May 2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               job_id               job_title  \\\n",
       "0   JOB-2019-0089696   Mid Level Data Analyst   \n",
       "1   JOB-2019-0089321        Platform Engineer   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.mycareersfuture.sg/job/mid-level-d...   \n",
       "1  https://www.mycareersfuture.sg/job/platform-en...   \n",
       "\n",
       "                                            job_desc  \\\n",
       "0  Data Analyst at Traveloka is at the forefront ...   \n",
       "1  Responsible for programming and maintaining mo...   \n",
       "\n",
       "                                             job_req           yrs_exp  \\\n",
       "0  We are looking for someone with:   Passion in ...  Junior Executive   \n",
       "1   Coding/Scripting experience in languages such...         Executive   \n",
       "\n",
       "                    indus                          comp         sal_amt  \\\n",
       "0  Information Technology  TRAVELOKA SERVICES PTE. LTD.  $4,300to$7,600   \n",
       "1  Information Technology           GUMI ASIA PTE. LTD.  $5,000to$6,500   \n",
       "\n",
       "  sal_term   emp_type         num_app         posted_date  \\\n",
       "0  Monthly  Permanent  2 applications  Posted 26 Apr 2019   \n",
       "1  Monthly  Full Time   0 application  Posted 26 Apr 2019   \n",
       "\n",
       "                 exp_date  \n",
       "0  Closing on 26 May 2019  \n",
       "1  Closing on 26 May 2019  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dataframe\n",
    "jobs_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "665"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if all the jobs extracted are unique listings\n",
    "\n",
    "len(jobs_df['job_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "### Export DataFrame to a CSV File\n",
    "\n",
    "</font>\n",
    "\n",
    "This can then be stored and loaded in a separate notebook for my study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "\n",
    "jobs_df.to_csv('./MyCareerSG.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
